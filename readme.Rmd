---
title: "lazyeval exercises"
author: "Tristan Mahr"
date: "June 18, 2016"
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  collapse = TRUE, 
  comment = "#>", 
  error = TRUE)
library("lazyeval")
```

Hadley Wickham has included exercises in the [lazyeval vignette](https://cran.rstudio.com/web/packages/lazyeval/vignettes/lazyeval.html).
This is a great idea. Normally, when reading a vignette, I spend most of my time
and attention playing with the example code. Exercises encourage this 
exploration by giving the reader problems to solve. Let's hope he's not 
[trolling us here](https://twitter.com/hadleywickham/status/743177000349667328).


## Part 1: Labeling

### Notes

`expr_text(x)` replaces `deparse(substitute(x))` idiom for capturing expressions
for labeling. `expr_label` is a more suitable version for printing messages.

`expr_find` improves on `substitute` by finding the original expression used for
a value; `substitute` only looks in its parent environment.

```{r}
f <- function(x) g(x)
g <- function(y) h(y)
h <- function(z) list(substitute = substitute(z), expr_find = expr_find(z))

f(1 + 2 + 3)
```

### Exercises

**`plot()` uses `deparse(substitute(x))` to generate labels for the x and y
axes. Can you generate input that causes it to display bad labels?**

Wrap the function so the user's arguments are stored in an intermediate
variable. That will undermine `substitute`'s search.

```{r}
par(mar = c(4.5, 4.5, 1, 0.5))
grid <- seq(0, 2 * pi, length = 100)

line_plot <- function(x, y, ...) {
  plot(x, y, type = "l", ...)
}

line_plot(grid, sin(grid))
```

**Write your own wrapper around `plot()` that uses `expr_label()` to compute
`xlim` and `ylim`.**

Assuming the question means `xlab` and `ylab`...

```{r}
line_plot2 <- function(x, y, ...) {
  plot(x, y, type = "l", xlab = expr_label(x), ylab = expr_label(y), ...)
}

line_plot2(grid, sin(grid))
```

**Create a simple implementation of `mean()` that stops with an informative
error message if the argument is not numeric.**

```{r}
x <- 1:100
my_mean <- function(x, ...) {
  if (!is.numeric(x)) {
    stop(expr_label(x), " is not a numeric vector.", call. = FALSE)  
  }
  mean(x, ...)
}

x <- c("a", "b", "c")
my_mean(x)
my_mean(x == "a")
my_mean("a")
```

**Read the source code for `expr_text()`. How does it work? What additional
arguments to `deparse()` does it use?**

Step one: It finds the original expression, then dispatches to a standard
evaluation version of the function.

```{r}
expr_text
```

Step two: It deparses the expression(s) into text and truncates it. The function
cuts off all text beyond a given number of characters (60 by default). Deparsing
produces a vector of text when the expression spans multiple lines. This
function can optionally cut off all text beyond a certain number lines. All the
lines of text are collapsed into a single string.

```{r}
lazyeval:::expr_text_
```

## Part 2: Formulas

### Exercises

**Create a wrapper around `lm()` that allows the user to supply the response and
predictors as two separate formulas.**

```{r}
lm2 <- function(response, predictor, data, ...) {
  # I can't figure out how to uq both at once within a formula that uses a
  # formula (~ lm(y ~ x)), so I'll plug them into a formula one by one.
  f <- y ~ x
  f_lhs(f) <- uq(response)
  f_rhs(f) <- uq(predictor)

  lm(formula = f, data = data, ...)
}

lm2(~ mpg, ~ cyl, mtcars)
lm2(~ mpg, ~ hp * cyl, mtcars)
```

**Compare and contrast `f_eval()` with `with()`.**

Both can use names outside of the data.

```{r}
cyl_sq <- mtcars$cyl ^ 2

with(mtcars, coef(lm(mpg ~ cyl + cyl_sq)))

f_eval(~ coef(lm(mpg ~ cyl + cyl_sq)), mtcars)
```

I thought `with` would work better with blocks of (multiple lines of) code,
because that's how I've seen `with` used. But `f_eval` can do those too.

```{r}
with(mtcars, {
  log_wt <- log(wt)
  mean(log_wt)
})

f_eval(~ {
  log_wt <- log(wt)
  mean(log_wt)
}, mtcars)
```

The main difference is that `f_eval` provides an unquoting/interpolation step in
the middle.

```{r}
some_var <- ~ wt
with(mtcars, mean(uq(some_var)))

f_eval(~ mean(uq(some_var)), mtcars)
```

This step difference is transparent in the function code. `with` jumps right 
into evaluation (`eval`).

```{r}
with.default
```

`f_eval` modifies the expression with `f_interp` before calling the evaluation
step (`eval_expr`).

```{r}
f_eval
```

**Why does this code work even though `f` is defined in two places? (And one of 
them is not a function).**

> ```{r}
> f <- function(x) x + 1
> f_eval(~ f(10), list(f = "a"))
> #> [1] 11
> ```

The same reason this can work coherently:

```{r}
log(100)
log <- 100
log(100)
log(log)
```

I don't know the low-level [eval/apply](https://groups.csail.mit.edu/mac/classes/6.001/abelson-sussman-lectures/wizard.jpg) 
steps for R in detail, but the interpreter looks for _a function_ with the name
`f` when evaluating a function call by `f`, so it skips over the piece of data
named `f` that is not a function.


```{r}
f <- function(x) x + 1
f_eval(~ f(10))
f_eval(~ f(10), data = list(f = "a"))

# Put a function in the data
f_eval(~ f(10), data = list(f = sqrt))
```



## Part 3: Non-standard scoping

### Exercises

**Write a function that selects all rows of `df` where variable is greater
than its mean. Make the function more general by allowing the user to specify 
a function to use instead of `mean()` (e.g. `median()`).**

Because we know the function is going to be `mean` we can add an assertion that
throws in an error if the variable has the wrong class. 

```{r}
above_average <- function(df, variable) {
  # variable should be average-able
  var_class <- f_eval(~ class(uq(variable)), df)
  if (!(var_class %in% c("logical", "numeric"))) {
    # Part 1
    stop(expr_label(variable), " cannot be averaged.", call. = FALSE)
  }
  
  rows <- f_eval(~ mean(uq(variable)) < uq(variable), df)
  rows[is.na(rows)] <- FALSE
  df[rows, , drop = FALSE]
}

mtcars$Model <- row.names(mtcars)
row.names(mtcars) <- NULL

above_average(mtcars, ~ Model)

above_average(mtcars, ~ hp)

# bottom half
above_average(mtcars, ~ -hp)
```

In the general case, we need to look up the function in the environment, not the
data. We will also add in dots to support for function argument like `na.rm`. 
The main function fits on one-line, but it is pretty tough to read.

```{r}
above_f <- function(df, variable, f, ...) {
  rows <- f_eval(~ .env$f(uq(variable), ...) < uq(variable), df)
  rows[is.na(rows)] <- FALSE
  df[rows, , drop = FALSE]
}

above_f(mtcars, ~ mpg, median)
above_f(mtcars, ~ mpg, function(xs, probs) quantile(xs, probs = probs), .75)
above_f(mtcars, ~ mpg, function(xs, probs) quantile(xs, probs = probs), .95)
above_f(mtcars, ~ mpg, max)
```



**Create a version of `mogrify()` where the first argument is `x`? What happens 
if you try to create a new variable called `x`?**

This is the original version of the `mogrify` function. The first argument has
a special syntactically obscured name to avoid matching any of the generated
column names. This exercise asks us to remove this safeguard. The
expected behavior is given below:

```{r}
mogrify1 <- function(`_df`, ...) {
  args <- f_list(...)
  for (nm in names(args)) `_df`[[nm]] <- f_eval(args[[nm]], `_df`)
  `_df`
}

df <- data.frame(y = 1:5)
mogrify1(df, z = ~ y * 2, x = ~ y * -1)
```

This is the updated version. I added a `str(args)` line to debug the structure
of the named list being looped over.

```{r}
mogrify2 <- function(x, ...) {
  args <- f_list(...)
  str(args)
  for (nm in names(args)) x[[nm]] <- f_eval(args[[nm]], x)
  x
}
```

We get an error when get try to create a column named `x`. The error that `f` is
not a formula matches the `str(args)` print-out, because it shows a data-frame
as one of the elements in `args`.

```{r}
rm(x)
mogrify2(df, z = ~ y * 2, x = ~ y * -1)
```

The problem is that the function arguments are being matched by name, not by
position. The same problem is on display here:

```{r}
f <- function(a, ...) c(a, ...)
f(1, 2, 3, 100)
f(1, 2, 3, a = 100)
```



## Part 4: Non-standard evaluation

### Notes

Nonstandard evaluation functions---which can use a bare expression, like dplyr
verbs---should dispatch to standard evaluation version that uses formulas. Use
`f_capture` to capture the expression in a formula.

### Exercises

**Recreate `subscramble()` using `base::subset()` instead of `sieve()`. Why does
it fail?**

